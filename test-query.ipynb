{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1718310939971
        }
      },
      "outputs": [],
      "source": [
        "# Notebook to perform a RAG query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1718311228117
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import re \n",
        "import json\n",
        "import pickle\n",
        "import math\n",
        "from datetime import datetime\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.core.credentials import AzureKeyCredential  \n",
        "from azure.search.documents import SearchClient  \n",
        "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient  \n",
        "from azure.search.documents.models import VectorizableTextQuery, VectorizedQuery\n",
        "from azure.search.documents.models import QueryType, QueryCaptionType, QueryAnswerType\n",
        "from openai import AzureOpenAI\n",
        "import openai\n",
        "from tenacity import retry, wait_random_exponential, stop_after_attempt \n",
        "import time\n",
        "import pandas as pd  \n",
        "import concurrent.futures  \n",
        "import random  \n",
        "from IPython.display import Markdown, display  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1718311233088
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "openai_temperature = 0.1\n",
        "\n",
        "#Load the configuration details for the AI Search Service and Azure OpenAI Instance\n",
        "#Credentials should be secured using a more secure method such as Azure KeyVault\n",
        "# Check if embedding settings are present\n",
        "def has_embedding_settings(config):\n",
        "    required_keys = [\n",
        "        'openai_embedding_api_base',\n",
        "        'openai_embedding_api_key', \n",
        "        'openai_embedding_api_version',\n",
        "        'openai_embedding_model'\n",
        "    ]\n",
        "    return all(key in config and config[key] for key in required_keys)\n",
        "\n",
        "if has_embedding_settings(config):\n",
        "    print(\"Embedding settings found. Proceeding with embedding operations...\")\n",
        "else:\n",
        "    print(\"Embedding settings not found or incomplete. Skipping embedding operations.\")\n",
        "    exit()\n",
        "\n",
        "# Azure AI Search Config\n",
        "search_service_name = config[\"search_service_name\"]\n",
        "search_service_url = \"https://{}.search.windows.net/\".format(search_service_name)\n",
        "search_admin_key = config[\"search_admin_key\"]\n",
        "index_name = config[\"search_index_name\"]\n",
        "search_api_version = config[\"search_api_version\"]\n",
        "\n",
        "#Azure OpenAI\n",
        "openai_embedding_api_base = config[\"openai_embedding_api_base\"]\n",
        "openai_embedding_api_key = config[\"openai_embedding_api_key\"]\n",
        "openai_embedding_api_version = config[\"openai_embedding_api_version\"]\n",
        "openai_embeddings_model = config[\"openai_embedding_model\"]\n",
        "\n",
        "openai_gpt_api_base = config[\"openai_gpt_api_base\"]\n",
        "openai_gpt_api_key = config[\"openai_gpt_api_key\"]\n",
        "openai_gpt_api_version = config[\"openai_gpt_api_version\"]\n",
        "openai_gpt_model = config[\"openai_gpt_model\"]\n",
        "\n",
        "# Check if all required AI Search variables are properly configured\n",
        "search_vars = [search_service_name, search_admin_key, index_name, search_api_version]\n",
        "ai_search_configured = True\n",
        "\n",
        "for var in search_vars:\n",
        "    if not var or var == \"[redacted]\" or var.strip() == \"\":\n",
        "        ai_search_configured = False\n",
        "        break\n",
        "\n",
        "if not ai_search_configured:\n",
        "    print(\"WARNING: AI Search variables not properly configured (some values are '[redacted]', empty, or None)\")\n",
        "    print(\"Search operations will not work properly. Please configure search_service_name, search_admin_key, search_index_name, and search_api_version.\")\n",
        "    index_client = None\n",
        "    search_client = None\n",
        "else:\n",
        "    index_client = SearchIndexClient(\n",
        "            endpoint=search_service_url, credential=AzureKeyCredential(search_admin_key))\n",
        "    search_client = SearchClient(endpoint=search_service_url, index_name=index_name, credential=AzureKeyCredential(search_admin_key))\n",
        "\n",
        "# gets the API Key from environment variable AZURE_OPENAI_API_KEY\n",
        "embeddings_client = AzureOpenAI(\n",
        "    api_version=openai_embedding_api_version,\n",
        "    azure_endpoint=openai_embedding_api_base,\n",
        "    api_key=openai_embedding_api_key\n",
        ")\n",
        "\n",
        "gpt_client = AzureOpenAI(\n",
        "    api_version=openai_gpt_api_version,\n",
        "    azure_endpoint=openai_gpt_api_base,\n",
        "    api_key=openai_gpt_api_key\n",
        ")\n",
        "\n",
        "print ('Search Service Name:', search_service_name)\n",
        "print ('Index Name:', index_name)\n",
        "print ('Azure OpenAI Embeddings Base URL:', openai_embedding_api_base)\n",
        "print ('Azure OpenAI Embeddings Model:', openai_embeddings_model)\n",
        "print ('Azure OpenAI GPT Base URL:', openai_gpt_api_base)\n",
        "print ('Azure OpenAI GPT Model:', openai_gpt_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1718311237228
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "max_tokens=2048\n",
        "\n",
        "# Function to generate vectors for title and content fields, also used for query vectors\n",
        "max_attempts = 6\n",
        "max_backoff = 60\n",
        "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(max_attempts))\n",
        "def generate_embedding(text):\n",
        "    if text == None:\n",
        "        return None\n",
        "        \n",
        "    if len(text) < 10:\n",
        "        return None\n",
        "        \n",
        "    # Initialize OpenAI client only if embedding settings are present\n",
        "    if has_embedding_settings(config):\n",
        "        client = openai.AzureOpenAI(\n",
        "            azure_endpoint=config[\"openai_embedding_api_base\"],\n",
        "            api_key=config[\"openai_embedding_api_key\"],\n",
        "            api_version=config[\"openai_embedding_api_version\"]\n",
        "        )\n",
        "    else:\n",
        "        print(\"Embedding settings not available. Cannot initialize OpenAI client for embeddings.\")\n",
        "        client = None    \n",
        "    counter = 0\n",
        "    incremental_backoff = 1   # seconds to wait on throttline - this will be incremental backoff\n",
        "    while True and counter < max_attempts:\n",
        "        try:\n",
        "            # text-embedding-3-small == 1536 dims\n",
        "            response = client.embeddings.create(\n",
        "                input=text,\n",
        "                model=openai_embeddings_model\n",
        "            )\n",
        "            return json.loads(response.model_dump_json())[\"data\"][0]['embedding']\n",
        "        except openai.APIError as ex:\n",
        "            # Handlethrottling - code 429\n",
        "            if str(ex.code) == \"429\":\n",
        "                incremental_backoff = min(max_backoff, incremental_backoff * 1.5)\n",
        "                print ('Waiting to retry after', incremental_backoff, 'seconds...')\n",
        "                time.sleep(incremental_backoff)\n",
        "            elif str(ex.code) == \"content_filter\":\n",
        "                print ('API Error', ex.code)\n",
        "                return None\n",
        "        except Exception as ex:\n",
        "            counter += 1\n",
        "            print ('Error - Retry count:', counter, ex)\n",
        "    return None\n",
        "\n",
        "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
        "def generate_answer(question, content):\n",
        "    max_attempts = 6\n",
        "    max_backoff = 60\n",
        "    system_prompt = \"\"\"\n",
        "    You are an intelligent assistant. \n",
        "    Use 'you' to refer to the individual asking the questions even if they ask with 'I'. \n",
        "    Sometimes the answer may be in a table.\n",
        "    Focus the response on the intent of the users question. For example, if they ask \"Who is\", aim to respond with information about \"Who\" as opposed to \"How to\".\n",
        "    Each source has a name followed by colon and the actual information. \n",
        "    Use square brackets to reference the source, for example [info1.txt]. \n",
        "    List each source separately, for example [info1.txt][info2.pdf].\n",
        "    For every fact, always include a reference to the source of that fact, even if you used the source to infer the fact.\n",
        "    Aim to be succint, but include any relevent information you find in the content such as special rules, legalities, restrictions or other relevent notes.\n",
        "    Only answer the question using the source information below. \n",
        "    Do not make up an answer.\n",
        "    \"\"\"\n",
        "\n",
        "    user_prompt = question + \"\\nSources:\\n\" + content\n",
        "\n",
        "    counter = 0\n",
        "    incremental_backoff = 1   # seconds to wait on throttline - this will be incremental backoff\n",
        "    while True and counter < max_attempts:\n",
        "        try:\n",
        "            response = gpt_client.chat.completions.create(\n",
        "                model=openai_gpt_model, \n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": user_prompt}\n",
        "                ],\n",
        "                temperature=openai_temperature,\n",
        "                max_tokens=max_tokens,\n",
        "                top_p=0.95,\n",
        "                frequency_penalty=0,\n",
        "                presence_penalty=0,\n",
        "                stop=None,\n",
        "                stream=False\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "            # elapsed_time = 0\n",
        "            # answer = ''\n",
        "            # for chunk in response:\n",
        "            #     if len(chunk.choices) > 0 and chunk.choices[0].delta.content != None:\n",
        "            #         answer += chunk.choices[0].delta.content\n",
        "            #         print(chunk.choices[0].delta.content, end='')\n",
        "\n",
        "\n",
        "            # return answer\n",
        "        except openai.APIError as ex:\n",
        "            # Handlethrottling - code 429\n",
        "            if str(ex.code) == \"429\":\n",
        "                incremental_backoff = min(max_backoff, incremental_backoff * 1.5)\n",
        "                print ('Waiting to retry after', incremental_backoff, 'seconds...')\n",
        "                time.sleep(incremental_backoff)\n",
        "            elif str(ex.code) == \"content_filter\":\n",
        "                print ('API Error', ex.code)\n",
        "                return \"\"\n",
        "        except Exception as ex:\n",
        "            counter += 1\n",
        "            print ('Error - Retry count:', counter, ex)\n",
        "        \n",
        "        return \"\"\n",
        "\n",
        "citation_pattern = r'\\[([^\\]]+)\\]'  \n",
        "def extract_citations(text):\n",
        "    citations = re.findall(citation_pattern, answer)  \n",
        "    return citations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1718311242184
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The intelligent cloud revenue for FY24 Q2 was approximately $25.88 billion [6ba17e30-a320-49a5-9d44-c0f96a0e2869-10]."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Pg 10\n",
        "query = \"What was the intelligent cloud revenue in FY24 Q2\"\n",
        "\n",
        "neighbors=3\n",
        "emb = generate_embedding(query)\n",
        "vector_query = VectorizedQuery(vector=emb, k_nearest_neighbors=neighbors, fields=\"vector\")\n",
        "results = search_client.search(  \n",
        "    search_text=query,  \n",
        "    vector_queries= [vector_query],\n",
        "    select=[\"doc_id, page_number, content\"],\n",
        "    top=neighbors,\n",
        "    query_type='semantic', \n",
        "    semantic_configuration_name='vector-semantic-configuration'\n",
        ")\n",
        "\n",
        "content = ''\n",
        "for result in results:  \n",
        "    content += result['doc_id'] + ': ' + result['content'] + '\\n\\n'\n",
        "    \n",
        "answer = generate_answer(query, content)    \n",
        "display(Markdown(answer))    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1718311243494
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "\\```markdown\n",
              "# Intelligent Cloud Overview\n",
              "\n",
              "## Investor Metrics\n",
              "\n",
              "|                        | FY23 Q3 | FY23 Q4 | FY24 Q1 | FY24 Q2 | FY24 Q3 |\n",
              "|------------------------|---------|---------|---------|---------|---------|\n",
              "| Server products and cloud services revenue growth (y/y) | 17% / 21%| 17% / 18%| 21%| 22% / 20%| 24%|\n",
              "\n",
              "*Growth rates include non-GAAP CC growth (GAAP % / CC %).*\n",
              "\n",
              "## Total Revenue\n",
              "\n",
              "- Revenue grew 21% driven by Azure\n",
              "\n",
              "## Operating Income\n",
              "\n",
              "- Gross margin dollars grew 20% and gross margin percentage decreased slightly. Excluding the impact of the latest change in accounting estimate for useful lives, gross margin percentage increased slightly primarily driven by improvement in Azure, inclusive of scaling our AI infrastructure, partially offset by sales mix shift to Azure.\n",
              "- Operating expenses grew 1% driven by investments in Azure\n",
              "- Operating income grew 32%\n",
              "\n",
              "## Revenue and Operating Income (in billions)\n",
              "\n",
              "| Period   | Revenue | Operating Income | \n",
              "|----------|---------|------------------| \n",
              "| FY23 Q3  | $22.08  | $9.48            |\n",
              "| FY23 Q4  | $23.99  | $10.53           |\n",
              "| FY24 Q1  | $24.26  | $11.75           |\n",
              "| FY24 Q2  | $25.88  | $12.46           |\n",
              "| FY24 Q3  | $26.71  | $12.51           |\n",
              "\n",
              "*Note: Numbers are approximate.*\n",
              "\n",
              "---\n",
              "\n",
              "*We have recast certain prior period amounts to conform to the way we internally manage and monitor our business. Includes non-GAAP constant currency (\"CC\") growth. See Appendix for reconciliation of GAAP and non-GAAP measures. Growth rates in GAAP and CC are equivalent unless otherwise noted.*\n",
              "\\```"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print out all the citations\n",
        "from IPython.display import Markdown, display  \n",
        "\n",
        "citations = sorted(list(set(extract_citations(answer))))\n",
        "for citation in citations:\n",
        "    # Perform the lookup query  \n",
        "    result = search_client.get_document(key=citation)  \n",
        "    \n",
        "    content = result.get('content', 'Content field not found')  \n",
        "    content = content.replace('```', '\\\\```')    \n",
        "    display(Markdown(content))  \n",
        "\n",
        "    print ('\\n==============================================================\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1718119566690
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
